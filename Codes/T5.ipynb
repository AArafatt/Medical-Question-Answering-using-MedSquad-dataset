{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7639866,"sourceType":"datasetVersion","datasetId":841565}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets openai","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n\ndata = pd.read_csv('/kaggle/input/layoutlm/medquad.csv')\n\ndata['answer'].fillna(\"No Answer\", inplace=True)\n\ndef preprocess_data(row):\n    return {\n        'input_text': f\"question: {row['question']} </s>\",\n        'target_text': row['answer'] + \" </s>\"\n    }\n\n\nformatted_data = data.apply(preprocess_data, axis=1)\n\nhf_dataset = Dataset.from_pandas(pd.DataFrame(formatted_data.tolist()))\nhf_dataset = hf_dataset.train_test_split(test_size=0.1)  # Split into training and validation sets\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T18:08:08.865947Z","iopub.execute_input":"2024-10-20T18:08:08.866356Z","iopub.status.idle":"2024-10-20T18:08:09.980834Z","shell.execute_reply.started":"2024-10-20T18:08:08.866309Z","shell.execute_reply":"2024-10-20T18:08:09.980070Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2805971981.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data['answer'].fillna(\"No Answer\", inplace=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import T5Tokenizer\n\n\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n\n\ndef tokenize_data(example):\n    source = tokenizer(example['input_text'], padding=\"max_length\", truncation=True, max_length=512)\n    target = tokenizer(example['target_text'], padding=\"max_length\", truncation=True, max_length=512)\n\n    return {\n        'input_ids': source['input_ids'],\n        'attention_mask': source['attention_mask'],\n        'labels': target['input_ids']  \n    }\n\ntokenized_dataset = hf_dataset.map(tokenize_data, batched=True)\ntokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T18:08:36.917149Z","iopub.execute_input":"2024-10-20T18:08:36.917532Z","iopub.status.idle":"2024-10-20T18:09:19.160077Z","shell.execute_reply.started":"2024-10-20T18:08:36.917495Z","shell.execute_reply":"2024-10-20T18:09:19.159162Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33855e9df3444ad2a7de808f7430888a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61e861015b9a44d3a54073931fd03b0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f852627b76e941dd955b80a27b5266a9"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14770 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf6db8ca5c504f3a9f1b1b66dbbecd8e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1642 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abbef6279eaf44a894ecd3178cb929c5"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./t5-finetuned-medquad\",\n    overwrite_output_dir=True,\n    eval_strategy=\"epoch\",  \n    learning_rate=3e-5,\n    per_device_train_batch_size=2, \n    per_device_eval_batch_size=2,   \n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_total_limit=2,\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['test'],\n)\n\n\ntrainer.train()\n\n\ntrainer.save_model(\"./t5-finetuned-medquad\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T18:12:42.690139Z","iopub.execute_input":"2024-10-20T18:12:42.690528Z","iopub.status.idle":"2024-10-20T20:31:51.809848Z","shell.execute_reply.started":"2024-10-20T18:12:42.690487Z","shell.execute_reply":"2024-10-20T20:31:51.808835Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='22155' max='22155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [22155/22155 2:19:04, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.022700</td>\n      <td>0.932083</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.846600</td>\n      <td>0.884382</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.842800</td>\n      <td>0.872469</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\n# Ensure the model is moved to the correct device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Function to generate an answer using T5\ndef generate_answer_t5(question):\n    input_text = f\"question: {question}\"  # Avoid manually adding </s>\n    \n    # Tokenize the input and move the input to the correct device\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    input_ids = inputs[\"input_ids\"].to(device)  # Move input_ids to the same device as the model\n    attention_mask = inputs[\"attention_mask\"].to(device)  # Move attention_mask to the same device\n\n    # Generate the answer\n    outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=150,\n        num_beams=5,\n        early_stopping=True\n    )\n\n    # Decode the generated text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return generated_text\n\n# Example usage on a single question from the dataset\nquestion_example = \"What is glaucoma?\"\ngenerated_answer = generate_answer_t5(question_example)\nprint(f\"Question: {question_example}\")\nprint(f\"Generated Answer: {generated_answer}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-20T20:33:15.509000Z","iopub.execute_input":"2024-10-20T20:33:15.509398Z","iopub.status.idle":"2024-10-20T20:33:18.443212Z","shell.execute_reply.started":"2024-10-20T20:33:15.509364Z","shell.execute_reply":"2024-10-20T20:33:18.442043Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Question: What is glaucoma?\nGenerated Answer: Glaucoma is a condition that affects the eyes. It is caused by glaucoma, which is a type of glaucoma that affects the eyes. It is a condition that affects the eye's ability to see clearly. Glaucoma is a type of glaucoma that affects the eye's ability to see clearly. Glaucoma is a type of glaucoma that affects the eye's ability to see clearly. Glaucoma is a condition that affects the eye's ability to see clearly. Glaucoma is a condition that affects the eye's ability to see clearly\n","output_type":"stream"}]}]}