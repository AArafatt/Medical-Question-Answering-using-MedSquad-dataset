{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7639866,"sourceType":"datasetVersion","datasetId":841565}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n\ndata = pd.read_csv('/kaggle/input/layoutlm/medquad.csv')\n\n\ndata = data.dropna(subset=['question', 'answer'])\n\n\nprint(data[['question', 'answer']].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T17:46:39.839022Z","iopub.execute_input":"2024-10-20T17:46:39.840132Z","iopub.status.idle":"2024-10-20T17:46:40.063635Z","shell.execute_reply.started":"2024-10-20T17:46:39.840089Z","shell.execute_reply":"2024-10-20T17:46:40.062610Z"}},"outputs":[{"name":"stdout","text":"                                 question  \\\n0                What is (are) Glaucoma ?   \n1                  What causes Glaucoma ?   \n2     What are the symptoms of Glaucoma ?   \n3  What are the treatments for Glaucoma ?   \n4                What is (are) Glaucoma ?   \n\n                                              answer  \n0  Glaucoma is a group of diseases that can damag...  \n1  Nearly 2.7 million people have glaucoma, a lea...  \n2  Symptoms of Glaucoma  Glaucoma can develop in ...  \n3  Although open-angle glaucoma cannot be cured, ...  \n4  Glaucoma is a group of diseases that can damag...  \n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n# Load your dataset (replace with the path to your dataset)\ndata = pd.read_csv('/kaggle/input/layoutlm/medquad.csv')\n\n# Fill missing values (if any) with a placeholder\ndata['answer'].fillna(\"No Answer\", inplace=True)\n\n# Prepare the dataset with input (question) and target (answer)\ndef preprocess_data(row):\n    return {\n        'input_text': f\"Q: {row['question']} A:\",\n        'target_text': row['answer']\n    }\n\n# Apply the preprocessing function to the dataset\nformatted_data = data.apply(preprocess_data, axis=1)\n\n# Convert to Hugging Face Dataset format\nhf_dataset = Dataset.from_pandas(pd.DataFrame(formatted_data.tolist()))\nhf_dataset = hf_dataset.train_test_split(test_size=0.1)  # Split into training and validation sets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T17:53:35.489800Z","iopub.execute_input":"2024-10-20T17:53:35.490190Z","iopub.status.idle":"2024-10-20T17:53:36.088410Z","shell.execute_reply.started":"2024-10-20T17:53:35.490153Z","shell.execute_reply":"2024-10-20T17:53:36.087400Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2005777306.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data['answer'].fillna(\"No Answer\", inplace=True)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from transformers import GPT2Tokenizer\n\n# Load the GPT-2 tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# Add the padding token (GPT-2 doesn't have one by default)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Tokenize the dataset\ndef tokenize_data(example):\n    source = tokenizer(example['input_text'], padding=\"max_length\", truncation=True, max_length=512)\n    target = tokenizer(example['target_text'], padding=\"max_length\", truncation=True, max_length=512)\n    \n    return {\n        'input_ids': source['input_ids'],\n        'attention_mask': source['attention_mask'],\n        'labels': target['input_ids']  # GPT-2 uses the same tokenizer for both input and output\n    }\n\n# Apply the tokenization to the dataset\ntokenized_dataset = hf_dataset.map(tokenize_data, batched=True)\ntokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T17:53:54.206082Z","iopub.execute_input":"2024-10-20T17:53:54.206480Z","iopub.status.idle":"2024-10-20T17:54:46.024162Z","shell.execute_reply.started":"2024-10-20T17:53:54.206442Z","shell.execute_reply":"2024-10-20T17:54:46.022935Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14770 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf671f3ca02249a182c066424709adad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1642 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b125d6467289464a846fcf095c0495b4"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n\n# Load the GPT-2 model\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./gpt2-finetuned-medquad\",\n    overwrite_output_dir=True,\n    eval_strategy=\"steps\",  # Changed from \"evaluation_strategy\" to \"eval_strategy\"\n    save_strategy=\"steps\",  # Save the model after a specific number of steps\n    eval_steps=500,         # Evaluate every 500 steps\n    save_steps=500,         # Save the model every 500 steps\n    learning_rate=5e-5,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['test'],\n)\n\n# Start fine-tuning the model\ntrainer.train()\n\n# Save the fine-tuned model\ntrainer.save_model(\"./gpt2-finetuned-medquad\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T17:57:16.275552Z","iopub.execute_input":"2024-10-20T17:57:16.276439Z","iopub.status.idle":"2024-10-20T19:46:59.308857Z","shell.execute_reply.started":"2024-10-20T17:57:16.276390Z","shell.execute_reply":"2024-10-20T19:46:59.307826Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114003811114041, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0dd5e8e8407454c8573478641a9f330"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241020_175912-u4r7vzc3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/arafatalam491-ahsanullah-university-of-science-technology/huggingface/runs/u4r7vzc3' target=\"_blank\">./gpt2-finetuned-medquad</a></strong> to <a href='https://wandb.ai/arafatalam491-ahsanullah-university-of-science-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/arafatalam491-ahsanullah-university-of-science-technology/huggingface' target=\"_blank\">https://wandb.ai/arafatalam491-ahsanullah-university-of-science-technology/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/arafatalam491-ahsanullah-university-of-science-technology/huggingface/runs/u4r7vzc3' target=\"_blank\">https://wandb.ai/arafatalam491-ahsanullah-university-of-science-technology/huggingface/runs/u4r7vzc3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='22155' max='22155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [22155/22155 1:47:41, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>3.465700</td>\n      <td>3.275872</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.998400</td>\n      <td>3.287554</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.766700</td>\n      <td>3.153626</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>3.393000</td>\n      <td>3.119809</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>3.895300</td>\n      <td>3.039443</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>2.779500</td>\n      <td>3.046404</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>2.849100</td>\n      <td>3.057050</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>2.762400</td>\n      <td>3.030465</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>2.830300</td>\n      <td>2.981403</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>2.889300</td>\n      <td>2.987535</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>2.508600</td>\n      <td>3.035266</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>2.739700</td>\n      <td>2.949336</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>3.797000</td>\n      <td>2.930159</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>3.129700</td>\n      <td>2.945387</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>3.556200</td>\n      <td>2.917093</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>3.435000</td>\n      <td>2.919744</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>2.990900</td>\n      <td>2.920871</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>3.270600</td>\n      <td>2.927754</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>3.656100</td>\n      <td>2.919312</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>2.856000</td>\n      <td>2.915909</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>2.736300</td>\n      <td>2.891351</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>3.370200</td>\n      <td>2.887673</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>2.423600</td>\n      <td>2.924936</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>3.195000</td>\n      <td>2.891002</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>2.635900</td>\n      <td>2.895656</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>3.342500</td>\n      <td>2.891940</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>3.193000</td>\n      <td>2.878788</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>3.017800</td>\n      <td>2.895705</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>2.637000</td>\n      <td>2.878807</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>3.750800</td>\n      <td>2.879382</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>3.209600</td>\n      <td>2.890272</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>3.127300</td>\n      <td>2.898086</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>2.790700</td>\n      <td>2.876418</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>3.428200</td>\n      <td>2.869236</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>2.604800</td>\n      <td>2.881852</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>3.302400</td>\n      <td>2.873544</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>3.018800</td>\n      <td>2.877826</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>2.587600</td>\n      <td>2.872257</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>3.700300</td>\n      <td>2.877988</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>2.611200</td>\n      <td>2.869591</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>3.916400</td>\n      <td>2.863001</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>3.666300</td>\n      <td>2.864109</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>3.392500</td>\n      <td>2.860113</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>2.975400</td>\n      <td>2.861693</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import torch\n\n# Ensure the model is moved to the correct device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Set the pad_token_id to eos_token_id (end of sentence token)\nif model.config.pad_token_id is None:\n    model.config.pad_token_id = model.config.eos_token_id\n\ndef generate_answer_gpt2(question):\n    input_text = f\"Q: {question} A:\"\n    \n    # Tokenize the input and move the input to the correct device\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    input_ids = inputs[\"input_ids\"].to(device)\n    attention_mask = inputs[\"attention_mask\"].to(device)\n\n    # Generate the answer with attention mask\n    outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=150,\n        num_beams=5,\n        early_stopping=True\n    )\n    \n    # Decode the generated text\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return generated_text\n\n# Example usage on a single question from your dataset\nquestion_example = \"What causes Glaucoma ?\"\ngenerated_answer = generate_answer_gpt2(question_example)\nprint(f\"Generated Answer: {generated_answer}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T19:55:20.650400Z","iopub.execute_input":"2024-10-20T19:55:20.651343Z","iopub.status.idle":"2024-10-20T19:55:22.150787Z","shell.execute_reply.started":"2024-10-20T19:55:20.651303Z","shell.execute_reply":"2024-10-20T19:55:22.149905Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Answer: Q: What causes Glaucoma ? A: glau glau glau glau glau glau glau gl glau glau glau glau glau glau glau glau glau glau glauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauauau\n","output_type":"stream"}],"execution_count":30}]}